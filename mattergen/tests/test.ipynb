{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26490dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mattergen.diffusion.diffusion_loss import *\n",
    "from mattergen.common.data.chemgraph import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cecc18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First graph: identity cell\n",
    "cell1 = torch.eye(3).unsqueeze(0)        # [1, 3, 3]\n",
    "pos1 = torch.zeros((1, 3))               # [1, 3]\n",
    "atomic_numbers1 = torch.tensor([1], dtype=torch.long)  # Hydrogen\n",
    "batch1 = torch.tensor([0], dtype=torch.long)           # batch index\n",
    "\n",
    "# Second graph: 2*identity cell\n",
    "cell2 = (2 * torch.eye(3)).unsqueeze(0)  # [1, 3, 3]\n",
    "pos2 = torch.zeros((1, 3))               # [1, 3]\n",
    "atomic_numbers2 = torch.tensor([1], dtype=torch.long)  # Hydrogen\n",
    "batch2 = torch.tensor([1], dtype=torch.long)           # batch index\n",
    "\n",
    "# Concatenate for batch\n",
    "cell = torch.cat([cell1, cell2], dim=0)               # [2, 3, 3]\n",
    "pos = torch.cat([pos1, pos2], dim=0)                  # [2, 3]\n",
    "atomic_numbers = torch.cat([atomic_numbers1, atomic_numbers2], dim=0)  # [2]\n",
    "batch = torch.cat([batch1, batch2], dim=0)            # [2]\n",
    "pbc = torch.tensor([1, 1, 1], dtype=torch.bool)        # periodic in all directions\n",
    "\n",
    "g = ChemGraph(\n",
    "    atomic_numbers=atomic_numbers,\n",
    "    pos=pos.requires_grad_(True),\n",
    "    cell=cell.requires_grad_(True),\n",
    "    batch=batch,\n",
    "    pbc=pbc\n",
    ")\n",
    "\n",
    "test = volume(g,None,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f082eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell:\n",
      " tensor([[[1., 0., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[2., 0., 0.],\n",
      "         [0., 2., 0.],\n",
      "         [0., 0., 2.]]])\n",
      "batch: tensor([0, 1])\n",
      "volume: tensor([1., 8.])\n"
     ]
    }
   ],
   "source": [
    "# First graph: identity cell\n",
    "cell1 = torch.eye(3).unsqueeze(0)        # [1, 3, 3]\n",
    "pos1 = torch.zeros((1, 3))               # [1, 3]\n",
    "atomic_numbers1 = torch.tensor([1], dtype=torch.long)  # Hydrogen\n",
    "batch1 = torch.tensor([0], dtype=torch.long)           # batch index\n",
    "\n",
    "# Second graph: 2*identity cell\n",
    "cell2 = (2 * torch.eye(3)).unsqueeze(0)  # [1, 3, 3]\n",
    "pos2 = torch.zeros((1, 3))               # [1, 3]\n",
    "atomic_numbers2 = torch.tensor([1], dtype=torch.long)  # Hydrogen\n",
    "batch2 = torch.tensor([1], dtype=torch.long)           # batch index\n",
    "\n",
    "# Concatenate for batch\n",
    "cell = torch.cat([cell1, cell2], dim=0)               # [2, 3, 3]\n",
    "pos = torch.cat([pos1, pos2], dim=0)                  # [2, 3]\n",
    "atomic_numbers = torch.cat([atomic_numbers1, atomic_numbers2], dim=0)  # [2]\n",
    "batch = torch.cat([batch1, batch2], dim=0)            # [2]\n",
    "pbc = torch.tensor([1, 1, 1], dtype=torch.bool)        # periodic in all directions\n",
    "\n",
    "g = ChemGraph(\n",
    "    atomic_numbers=atomic_numbers,\n",
    "    pos=pos,\n",
    "    cell=cell,\n",
    "    batch=batch,\n",
    "    pbc=pbc\n",
    ")\n",
    "\n",
    "print(\"cell:\\n\", g.cell)\n",
    "print(\"batch:\", g.batch)\n",
    "print(\"volume:\", torch.abs(torch.det(g.cell)))  # Should be tensor([1., 8.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2419d6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance = {\"volume\":5.0}\n",
    "loss_fn = make_combined_loss(guidance)\n",
    "t=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46147e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_tensor = torch.tensor(\n",
    "    [[[ 4.3516, -1.5141,  0.6149],\n",
    "      [-1.5141,  6.1434,  0.1679],\n",
    "      [ 0.6149,  0.1679,  5.4232]],\n",
    "\n",
    "     [[ 6.2720, -0.5779,  1.0578],\n",
    "      [-0.5779,  6.9124, -0.3065],\n",
    "      [ 1.0578, -0.3065,  6.9619]],\n",
    "\n",
    "     [[ 6.0133,  0.2140,  0.2133],\n",
    "      [ 0.2140,  5.0960,  1.3883],\n",
    "      [ 0.2133,  1.3883,  7.1379]],\n",
    "\n",
    "     [[ 5.8231,  0.3393, -0.5104],\n",
    "      [ 0.3393,  3.7942,  0.0349],\n",
    "      [-0.5104,  0.0349,  4.8177]],\n",
    "\n",
    "     [[ 4.5687,  0.6235, -1.2992],\n",
    "      [ 0.6235,  5.9437,  0.6961],\n",
    "      [-1.2992,  0.6961,  7.0461]]], device='cuda:0', requires_grad=True\n",
    ")\n",
    "\n",
    "pos_tensor = torch.tensor(\n",
    "    [[0.1310, 0.9461, 0.7470],\n",
    "     [0.4265, 0.1370, 0.1155],\n",
    "     [0.9801, 0.0569, 0.0659],\n",
    "     [0.8773, 0.0013, 0.9509],\n",
    "     [0.5542, 0.5153, 0.7079],\n",
    "     [0.9035, 0.7442, 0.4594],\n",
    "     [0.9149, 0.1001, 0.9507],\n",
    "     [0.3755, 0.4562, 0.8747],\n",
    "     [0.1470, 0.2804, 0.4319],\n",
    "     [0.6593, 0.6515, 0.3958],\n",
    "     [0.3811, 0.9338, 0.4557],\n",
    "     [0.0914, 0.7402, 0.1936],\n",
    "     [0.6704, 0.9788, 0.4947],\n",
    "     [0.4495, 0.4411, 0.0278],\n",
    "     [0.8079, 0.6355, 0.4580],\n",
    "     [0.9581, 0.0243, 0.2232],\n",
    "     [0.2198, 0.3100, 0.4565],\n",
    "     [0.9197, 0.3020, 0.3068],\n",
    "     [0.4468, 0.6582, 0.9835],\n",
    "     [0.3606, 0.5917, 0.1846],\n",
    "     [0.4044, 0.1283, 0.0664],\n",
    "     [0.6070, 0.2345, 0.2315],\n",
    "     [0.1035, 0.2380, 0.7021],\n",
    "     [0.2375, 0.0644, 0.8052],\n",
    "     [0.3153, 0.2525, 0.7850],\n",
    "     [0.5986, 0.3825, 0.3735],\n",
    "     [0.8002, 0.8277, 0.5344],\n",
    "     [0.0336, 0.7086, 0.8363],\n",
    "     [0.3363, 0.3324, 0.0386],\n",
    "     [0.1602, 0.4362, 0.1427],\n",
    "     [0.4245, 0.2435, 0.4621],\n",
    "     [0.8812, 0.3093, 0.1818],\n",
    "     [0.0933, 0.5245, 0.1296],\n",
    "     [0.0288, 0.3235, 0.9353],\n",
    "     [0.3725, 0.8614, 0.6027],\n",
    "     [0.4180, 0.9526, 0.2075],\n",
    "     [0.5338, 0.0342, 0.6897],\n",
    "     [0.1304, 0.3942, 0.8355],\n",
    "     [0.3617, 0.0796, 0.9171],\n",
    "     [0.2735, 0.9155, 0.5862],\n",
    "     [0.2335, 0.8733, 0.0771],\n",
    "     [0.4929, 0.7656, 0.8187],\n",
    "     [0.1524, 0.1886, 0.5731],\n",
    "     [0.1533, 0.0940, 0.1612],\n",
    "     [0.5566, 0.1702, 0.6329],\n",
    "     [0.0290, 0.9601, 0.7891],\n",
    "     [0.4740, 0.2412, 0.8689],\n",
    "     [0.2413, 0.8498, 0.2300],\n",
    "     [0.7346, 0.6523, 0.9365],\n",
    "     [0.2465, 0.2853, 0.0799],\n",
    "     [0.9466, 0.6985, 0.2798],\n",
    "     [0.5104, 0.7481, 0.7046],\n",
    "     [0.9831, 0.9127, 0.1403],\n",
    "     [0.6480, 0.4283, 0.1882],\n",
    "     [0.2928, 0.3257, 0.0968],\n",
    "     [0.0296, 0.1578, 0.6829],\n",
    "     [0.2803, 0.6835, 0.9186]], device='cuda:0', requires_grad=True\n",
    ")\n",
    "\n",
    "# Example atomic_numbers, batch, and pbc for demonstration\n",
    "atomic_numbers = torch.ones(pos_tensor.shape[0], dtype=torch.long, device='cuda:0')\n",
    "batch = torch.zeros(pos_tensor.shape[0], dtype=torch.long, device='cuda:0')\n",
    "pbc = torch.tensor([True, True, True], device='cuda:0')\n",
    "\n",
    "x = ChemGraphBatch(\n",
    "    atomic_numbers=atomic_numbers,\n",
    "    pos=pos_tensor,\n",
    "    cell=cell_tensor,\n",
    "    batch=batch,\n",
    "    pbc=pbc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8341e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = {\n",
    "    'cell': torch.tensor([[[ 0.9351,  1.3775, -0.5692],\n",
    "                          [ 1.3775, -0.6458, -0.1520],\n",
    "                          [-0.5692, -0.1520, -0.0249]],\n",
    "\n",
    "                         [[ 0.1604,  0.3677, -0.6655],\n",
    "                          [ 0.3677, -0.2679,  0.1940],\n",
    "                          [-0.6655,  0.1940, -0.2698]],\n",
    "\n",
    "                         [[-0.3833, -0.1860, -0.1694],\n",
    "                          [-0.1860,  0.4003, -1.1894],\n",
    "                          [-0.1694, -1.1894, -1.3410]],\n",
    "\n",
    "                         [[ 0.0578, -0.2550,  0.3932],\n",
    "                          [-0.2550,  1.6095, -0.0382],\n",
    "                          [ 0.3932, -0.0382,  0.8575]],\n",
    "\n",
    "                         [[ 0.8639, -0.5432,  1.1023],\n",
    "                          [-0.5432, -0.3252, -0.5981],\n",
    "                          [ 1.1023, -0.5981, -1.2547]]], device='cuda:0'),\n",
    "    'pos': torch.tensor([[-0.0142, -0.0097, -0.0061],\n",
    "                         [-0.0111, -0.0079, -0.0057],\n",
    "                         [-0.0106, -0.0072, -0.0058],\n",
    "                         [-0.0114, -0.0081, -0.0065],\n",
    "                         [-0.0085, -0.0064, -0.0074],\n",
    "                         [-0.0100, -0.0059, -0.0054],\n",
    "                         [-0.0089, -0.0071, -0.0061],\n",
    "                         [-0.0108, -0.0078, -0.0062],\n",
    "                         [-0.0094, -0.0076, -0.0063],\n",
    "                         [-0.0048, -0.0034, -0.0028],\n",
    "                         [-0.0058, -0.0058, -0.0040],\n",
    "                         [-0.0055, -0.0055, -0.0044],\n",
    "                         [-0.0042, -0.0033, -0.0032],\n",
    "                         [-0.0043, -0.0043, -0.0048],\n",
    "                         [-0.0062, -0.0054, -0.0036],\n",
    "                         [-0.0052, -0.0042, -0.0034],\n",
    "                         [-0.0047, -0.0034, -0.0020],\n",
    "                         [-0.0043, -0.0024, -0.0024],\n",
    "                         [-0.0046, -0.0046, -0.0053],\n",
    "                         [-0.0039, -0.0035, -0.0034],\n",
    "                         [-0.0031, -0.0034, -0.0054],\n",
    "                         [-0.0037, -0.0031, -0.0029],\n",
    "                         [-0.0044, -0.0028, -0.0027],\n",
    "                         [-0.0041, -0.0042, -0.0036],\n",
    "                         [-0.0033, -0.0028, -0.0038],\n",
    "                         [-0.0113, -0.0064, -0.0036],\n",
    "                         [-0.0097, -0.0085, -0.0043],\n",
    "                         [-0.0102, -0.0083, -0.0059],\n",
    "                         [-0.0066, -0.0073, -0.0040],\n",
    "                         [-0.0083, -0.0071, -0.0056],\n",
    "                         [-0.0106, -0.0089, -0.0049],\n",
    "                         [-0.0086, -0.0065, -0.0058],\n",
    "                         [-0.0105, -0.0067, -0.0045],\n",
    "                         [-0.0085, -0.0071, -0.0052],\n",
    "                         [-0.0079, -0.0068, -0.0049],\n",
    "                         [-0.0069, -0.0048, -0.0048],\n",
    "                         [-0.0042, -0.0029, -0.0040],\n",
    "                         [-0.0026, -0.0034, -0.0032],\n",
    "                         [-0.0059, -0.0026, -0.0019],\n",
    "                         [-0.0059, -0.0068, -0.0043],\n",
    "                         [-0.0035, -0.0059, -0.0040],\n",
    "                         [-0.0031, -0.0054, -0.0025],\n",
    "                         [-0.0044, -0.0054, -0.0042],\n",
    "                         [-0.0032, -0.0045, -0.0032],\n",
    "                         [-0.0050, -0.0024, -0.0040],\n",
    "                         [-0.0025, -0.0043, -0.0028],\n",
    "                         [-0.0041, -0.0022, -0.0041],\n",
    "                         [-0.0112, -0.0083, -0.0051],\n",
    "                         [-0.0101, -0.0087, -0.0054],\n",
    "                         [-0.0102, -0.0067, -0.0051],\n",
    "                         [-0.0090, -0.0082, -0.0044],\n",
    "                         [-0.0097, -0.0068, -0.0055],\n",
    "                         [-0.0123, -0.0066, -0.0052],\n",
    "                         [-0.0096, -0.0094, -0.0043],\n",
    "                         [-0.0097, -0.0080, -0.0048],\n",
    "                         [-0.0127, -0.0073, -0.0058],\n",
    "                         [-0.0126, -0.0077, -0.0064]], device='cuda:0')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9466884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cell: True \n",
      " pos: True\n",
      "IN LOSS: cell requires_grad: True\n",
      "IN LOSS: cell is_leaf: True\n",
      "IN LOSS: cell id: 139910436391408\n",
      "Diffusion loss: tensor([124.7910, 286.5561, 201.7103,  99.8801, 170.2235], device='cuda:0',\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN LOSS: cell requires_grad: True\n",
      "IN LOSS: cell is_leaf: True\n",
      "IN LOSS: cell id: 139910436391408\n",
      "IN LOSS: cell requires_grad: False\n",
      "IN LOSS: cell is_leaf: True\n",
      "IN LOSS: cell id: 139910435680992\n",
      "cell:\n",
      " tensor([[[1., 0., 0.],\n",
      "         [0., 1., 0.],\n",
      "         [0., 0., 1.]],\n",
      "\n",
      "        [[2., 0., 0.],\n",
      "         [0., 2., 0.],\n",
      "         [0., 0., 2.]]], requires_grad=True)\n",
      "batch: tensor([0, 1])\n",
      "volume: tensor([1., 8.], grad_fn=<AbsBackward0>)\n",
      "IN LOSS: cell requires_grad: True\n",
      "IN LOSS: cell is_leaf: True\n",
      "IN LOSS: cell id: 139910469742048\n",
      "IN LOSS: cell requires_grad: True\n",
      "IN LOSS: cell is_leaf: True\n",
      "IN LOSS: cell id: 139910461975552\n"
     ]
    }
   ],
   "source": [
    "replace_kwargs = [\"pos\", \"cell\"]\n",
    "            # Create a new ChemGraphBatch with requires_grad=True for pos and cell         \n",
    "x_for_grad = ChemGraphBatch(\n",
    "                atomic_numbers=x.atomic_numbers,\n",
    "                pos=x.pos.clone().detach().requires_grad_(True),\n",
    "                cell=x.cell.clone().detach().requires_grad_(True),\n",
    "                batch=x.batch,\n",
    "            )\n",
    "print(\"cell:\", x_for_grad.cell.requires_grad, \"\\n\", \"pos:\", x_for_grad.pos.requires_grad)\n",
    "\n",
    "grad_dict = {}\n",
    "diffusion_loss = loss_fn(x_for_grad, t)\n",
    "print(\"Diffusion loss:\", diffusion_loss)\n",
    "for field in replace_kwargs:\n",
    "    grad = torch.autograd.grad(\n",
    "        diffusion_loss, getattr(x_for_grad, field),\n",
    "        grad_outputs=torch.ones_like(diffusion_loss),\n",
    "        create_graph=True,\n",
    "        allow_unused=True\n",
    "    )[0]\n",
    "    if grad is None:\n",
    "        grad = torch.zeros_like(getattr(x_for_grad, field))\n",
    "    print(f\"Gradient for {field}:\", grad)\n",
    "    grad_dict[field] = grad\n",
    "\n",
    "print(\"------------------------------\\n\",\n",
    "      \"Diffusion loss gradient:\", grad_dict, \"\\n\",\n",
    "      \"Diffusion loss function:\", loss_fn(x_for_grad, None), \"\\n\",\n",
    "      loss_fn, \"\\n------------------------------\")\n",
    "for k in grad_dict:\n",
    "    if k in scores:\n",
    "        scores[k] = scores[k] - grad_dict[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1148089a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_for_grad.cell.requires_grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
